{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r'.\\Data\\train\\train.json'\n",
    "with open(file) as train_file:\n",
    "    dict_train = json.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sno = []\n",
    "cuisine = []\n",
    "ingredients = []\n",
    "for i in range(len(dict_train)):\n",
    "    sno.append(dict_train[i]['id'])\n",
    "    cuisine.append(dict_train[i]['cuisine'])\n",
    "    ingredients.append(dict_train[i]['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'id':sno, \n",
    "                   'cuisine':cuisine, \n",
    "                   'ingredients':ingredients})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzer(df):\n",
    "    new_df = []\n",
    "    for s in df:\n",
    "        s = ' '.join(s)\n",
    "        new_df.append(s) \n",
    "    import re\n",
    "    mod_df=[]\n",
    "    for s in new_df:\n",
    "        s=re.sub(r'[^\\w\\s]','',s)\n",
    "        s=re.sub(r\"(\\d)\", \"\", s)    \n",
    "        s=re.sub(r'\\([^)]*\\)', '', s)\n",
    "        s=re.sub(u'\\w*\\u2122', '', s)\n",
    "        s=s.lower()    \n",
    "        #Remove Stop Words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = word_tokenize(s)\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "        filtered_sentence = []\n",
    "        for w in word_tokens:\n",
    "            if w not in stop_words:\n",
    "                filtered_sentence.append(w)\n",
    "        s=' '.join(filtered_sentence)    \n",
    "        #Porter Stemmer Algorithm\n",
    "        words = word_tokenize(s)\n",
    "        word_ps=[]\n",
    "        for w in words:\n",
    "           word_ps.append(ps.stem(w))\n",
    "        s=' '.join(word_ps)\n",
    "        mod_df.append(s)\n",
    "    return mod_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzer1(df):\n",
    "    import re\n",
    "    mod_df=[]\n",
    "    for s in df:\n",
    "        s=re.sub(r'[^\\w\\s]','',s)\n",
    "        s=re.sub(r\"(\\d)\", \"\", s)\n",
    "        s=re.sub(r'\\([^)]*\\)', '', s)   \n",
    "        s=re.sub(u'\\w*\\u2122', '', s)\n",
    "        s=s.lower()\n",
    "        #Remove Stop Words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = word_tokenize(s)\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "        filtered_sentence = []\n",
    "        for w in word_tokens:\n",
    "            if w not in stop_words:\n",
    "                filtered_sentence.append(w)\n",
    "        s=' '.join(filtered_sentence)\n",
    "        #Porter Stemmer Algorithm\n",
    "        words = word_tokenize(s)\n",
    "        word_ps=[]\n",
    "        for w in words:\n",
    "           word_ps.append(ps.stem(w))\n",
    "        s=' '.join(word_ps)\n",
    "        mod_df.append(s)\n",
    "    return mod_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ingredients_mod']= analyzer(df['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist =['fish', 'goat', 'chicken','beef','pork','prawn','egg','Katsuobushi','mackrel','fillet','lamb','steak','salmon','shrimp']\n",
    "pattern = '|'.join(mylist)\n",
    "df['veg']=df.ingredients_mod.str.contains(pattern) \n",
    "df.loc[df.veg == True,'veg'] = 'non-vegetarian'\n",
    "df.loc[df.veg == False,'veg'] = 'vegetarian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import re\n",
    "file = r'.\\Data\\test\\test.csv'\n",
    "df_test = pd.read_csv(file,encoding='ISO-8859-1')\n",
    "df_test['ingredients_mod']= analyzer1(df_test['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "ingredient_train = vectorizer.fit_transform(df['ingredients_mod'])\n",
    "ingredient_test = vectorizer.transform(df_test['ingredients_mod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "elable = preprocessing.LabelEncoder()\n",
    "elable.fit(df['cuisine'])\n",
    "df['cuisine']=elable.transform(df['cuisine'])\n",
    "elable.fit(df['veg'])\n",
    "df['veg']=elable.transform(df['veg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_map={'0':'non-vegetarian', '1':'vegetarian'}\n",
    "cuisine_map={'0':'brazilian', '1':'british', '2':'cajun_creole', '3':'chinese', '4':'filipino', '5':'french', '6':'greek', '7':'indian', '8':'irish', '9':'italian', '10':'jamaican', '11':'japanese', '12':'korean', '13':'mexican', '14':'moroccan', '15':'russian', '16':'southern_us', '17':'spanish', '18':'thai', '19':'vietnamese'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cuisine_train=[]\n",
    "Type_train=[]\n",
    "Cuisine_train = df['cuisine']\n",
    "Type_train = df['veg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "lin_cuisine = svm.LinearSVC(C=1)\n",
    "lin_veg = svm.LinearSVC(C=1)\n",
    "lin_cuisine.fit(ingredient_train, Cuisine_train)\n",
    "lin_veg.fit(ingredient_train, Type_train)\n",
    "Cuisine_pred_idx=lin_cuisine.predict(ingredient_test)\n",
    "Type_pred_idx=lin_veg.predict(ingredient_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Actual Cuisine Predicted Cuisine  \\\n",
      "0                                   American pancakes       southern_us   \n",
      "1              Bourbon, black cherry & bacon brownies       southern_us   \n",
      "2                   Safe-to-eat uncooked cookie dough       southern_us   \n",
      "3   Tomato, avocado & corn salad with migas & butt...           mexican   \n",
      "4                                        Creamed corn           mexican   \n",
      "5                                         Pumpkin pie       southern_us   \n",
      "6                            American burnt onion dip           mexican   \n",
      "7                               Smoky braised brisket          moroccan   \n",
      "8                  Fully loaded Cajun chicken burgers          moroccan   \n",
      "9                                         S'mores dip            french   \n",
      "10                                   Texas BBQ medley       southern_us   \n",
      "11                                All-American T-bone           italian   \n",
      "12                                New York cheesecake       southern_us   \n",
      "13                  Smoky pork & Boston beans one-pot           italian   \n",
      "14                 Baby back ribs with Carolina baste       southern_us   \n",
      "15                  Spicy pies with sweet potato mash           russian   \n",
      "16                           John Torode's big burger            french   \n",
      "17                        Peanut butter jelly cookies           british   \n",
      "18                                    Buffalo chicken       southern_us   \n",
      "19             Stickiest ever BBQ ribs with chive dip           chinese   \n",
      "20                               Cheesesteak hot dogs           british   \n",
      "21      Sticky bourbon BBQ wings with blue cheese dip       southern_us   \n",
      "22                                Choc chip pecan pie       southern_us   \n",
      "23                                  Diner sloppy joes           mexican   \n",
      "24                                    BBQ pulled pork       southern_us   \n",
      "\n",
      "        Veg/NonVeg  \n",
      "0   non-vegetarian  \n",
      "1       vegetarian  \n",
      "2       vegetarian  \n",
      "3       vegetarian  \n",
      "4       vegetarian  \n",
      "5   non-vegetarian  \n",
      "6       vegetarian  \n",
      "7   non-vegetarian  \n",
      "8   non-vegetarian  \n",
      "9       vegetarian  \n",
      "10  non-vegetarian  \n",
      "11  non-vegetarian  \n",
      "12      vegetarian  \n",
      "13  non-vegetarian  \n",
      "14  non-vegetarian  \n",
      "15      vegetarian  \n",
      "16  non-vegetarian  \n",
      "17  non-vegetarian  \n",
      "18  non-vegetarian  \n",
      "19  non-vegetarian  \n",
      "20  non-vegetarian  \n",
      "21      vegetarian  \n",
      "22  non-vegetarian  \n",
      "23  non-vegetarian  \n",
      "24  non-vegetarian  \n"
     ]
    }
   ],
   "source": [
    "Cuisine_pred=[]\n",
    "for i in range(len(Cuisine_pred_idx)):\n",
    "    Cuisine_pred.append(cuisine_map[str(Cuisine_pred_idx[i])])\n",
    "       \n",
    "Type_pred=[]\n",
    "for i in range(len(Type_pred_idx)):\n",
    "    Type_pred.append(veg_map[str(Type_pred_idx[i])])    \n",
    "\n",
    "df_test['cuisine'] =  Cuisine_pred\n",
    "df_test['Veg/NonVeg'] =  Type_pred\n",
    "Result = pd.DataFrame({'Actual Cuisine':df_test['receipes'], 'Predicted Cuisine':Cuisine_pred, 'Veg/NonVeg':Type_pred})\n",
    "print(Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
